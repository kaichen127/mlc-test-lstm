{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing pseudocode\n",
    "\n",
    "# read from data_algebraic directory\n",
    "# parse the data, read exclusively from the support section\n",
    "# copy 1 random support example and label it as the query example\n",
    "# concatenate the support examples and the duplicate query example into a sentence\n",
    "# tokenize each word into a number, pad it to a fixed length\n",
    "\n",
    "# training pseudocode\n",
    "\n",
    "# split into training and validation set -> already done\n",
    "# feed the IN and OUT for each support example into the model\n",
    "# feed only the IN for the query, have the model predict the OUT\n",
    "# loss function to be determined ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file in the training set of 100000, parse the file\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "# for i in range(1, 100001):  \n",
    "    file_index = f\"{i:06d}\"\n",
    "    file_name = file_index + \".txt\"\n",
    "    file_path = f\"data_algebraic/train/{file_name}\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            \n",
    "            # logic to parse the file\n",
    "            sections = file_content.split('\\n\\n')\n",
    "            \n",
    "            support_section = None\n",
    "            \n",
    "            for section in sections:\n",
    "                if section.startswith('*SUPPORT*'):\n",
    "                    support_section = section.split('\\n')[1:]  # Skip the header line\n",
    "                    break \n",
    "            \n",
    "            if support_section is None:\n",
    "                print(f\"No *SUPPORT* section found in file {file_name}.\")\n",
    "                continue\n",
    "            \n",
    "            # Convert support data to a list of tuples (IN, OUT)\n",
    "            parsed_support_data = []\n",
    "            for line in support_section:\n",
    "                if line.startswith('IN:') and 'OUT:' in line:\n",
    "                    in_part = line.split('IN:')[1].split('OUT:')[0].strip()\n",
    "                    out_part = line.split('OUT:')[1].strip()\n",
    "                    parsed_support_data.append((in_part, out_part))\n",
    "            \n",
    "            data.append(parsed_support_data)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "\n",
    "# # Print the parsed data for verification\n",
    "# for item in data:\n",
    "#     for in_part, out_part in item:\n",
    "#         print(f\"IN: {in_part} OUT: {out_part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dax lug', 'GREEN GREEN GREEN GREEN'),\n",
       " ('fep dax dax fep kiki zup gazzer', 'PINK YELLOW GREEN GREEN YELLOW YELLOW'),\n",
       " ('wif dax kiki', 'PURPLE GREEN GREEN'),\n",
       " ('dax dax lug', 'GREEN GREEN GREEN GREEN GREEN'),\n",
       " ('fep dax fep kiki zup wif', 'PURPLE YELLOW GREEN YELLOW YELLOW'),\n",
       " ('fep wif lug zup dax', 'GREEN YELLOW PURPLE PURPLE PURPLE PURPLE'),\n",
       " ('dax', 'GREEN'),\n",
       " ('gazzer', 'PINK'),\n",
       " ('wif lug', 'PURPLE PURPLE PURPLE PURPLE'),\n",
       " ('fep kiki', 'YELLOW YELLOW'),\n",
       " ('dax zup dax', 'GREEN GREEN'),\n",
       " ('wif kiki zup wif', 'PURPLE PURPLE PURPLE'),\n",
       " ('wif zup gazzer', 'PINK PURPLE'),\n",
       " ('wif kiki', 'PURPLE PURPLE')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input symbol 'PINK' maps to token '16.0'\n",
      "Token '16.0' maps back to input symbol 'PINK'\n"
     ]
    }
   ],
   "source": [
    "# tokenizer dictionary\n",
    "\n",
    "input_list = ['SOS', 'dax', 'lug', 'wif', 'zup', 'fep', 'blicket', 'kiki', 'tufa', 'gazzer', 'EOS',]\n",
    "output_list = ['SOS','RED', 'YELLOW', 'GREEN', 'BLUE', 'PURPLE', 'PINK', 'EOS']\n",
    "vocab_list = ['SOS', 'dax', 'lug', 'wif', 'zup', 'fep', 'blicket', 'kiki', 'tufa', 'gazzer', 'RED', 'YELLOW', 'GREEN', 'BLUE', 'PURPLE', 'PINK',  'EOS',]\n",
    "\n",
    "input_symbol_to_token = {symbol: float(idx + 1) for idx, symbol in enumerate(vocab_list)}\n",
    "token_to_input_symbol = {float(idx + 1): symbol for idx, symbol in enumerate(vocab_list)}\n",
    "\n",
    "# example\n",
    "\n",
    "input_symbol = 'PINK'\n",
    "\n",
    "input_token = input_symbol_to_token[input_symbol]\n",
    "\n",
    "print(f\"Input symbol '{input_symbol}' maps to token '{input_token}'\")\n",
    "\n",
    "# Reverse mapping example:\n",
    "reverse_input_symbol = token_to_input_symbol[input_token]\n",
    "\n",
    "print(f\"Token '{input_token}' maps back to input symbol '{reverse_input_symbol}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the original data\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = ['SOS'] + sentence.split() + ['EOS']\n",
    "    token_ids = [input_symbol_to_token[token] for token in tokens]\n",
    "    return token_ids\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for sample in data:\n",
    "    tokenized_sample = []\n",
    "    for input_sentence, output_sentence in sample:\n",
    "        input_tokens = tokenize_sentence(input_sentence)\n",
    "        output_tokens = tokenize_sentence(output_sentence)\n",
    "        tokenized_sample.append((input_tokens, output_tokens))\n",
    "    tokenized_data.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1.0, 3.0, 4.0, 2.0], [1.0, 14.0, 14.0, 14.0, 14.0, 2.0]),\n",
       " ([1.0, 7.0, 3.0, 3.0, 7.0, 9.0, 6.0, 11.0, 2.0],\n",
       "  [1.0, 17.0, 13.0, 14.0, 14.0, 13.0, 13.0, 2.0]),\n",
       " ([1.0, 5.0, 3.0, 9.0, 2.0], [1.0, 16.0, 14.0, 14.0, 2.0]),\n",
       " ([1.0, 3.0, 3.0, 4.0, 2.0], [1.0, 14.0, 14.0, 14.0, 14.0, 14.0, 2.0]),\n",
       " ([1.0, 7.0, 3.0, 7.0, 9.0, 6.0, 5.0, 2.0],\n",
       "  [1.0, 16.0, 13.0, 14.0, 13.0, 13.0, 2.0]),\n",
       " ([1.0, 7.0, 5.0, 4.0, 6.0, 3.0, 2.0],\n",
       "  [1.0, 14.0, 13.0, 16.0, 16.0, 16.0, 16.0, 2.0]),\n",
       " ([1.0, 3.0, 2.0], [1.0, 14.0, 2.0]),\n",
       " ([1.0, 11.0, 2.0], [1.0, 17.0, 2.0]),\n",
       " ([1.0, 5.0, 4.0, 2.0], [1.0, 16.0, 16.0, 16.0, 16.0, 2.0]),\n",
       " ([1.0, 7.0, 9.0, 2.0], [1.0, 13.0, 13.0, 2.0]),\n",
       " ([1.0, 3.0, 6.0, 3.0, 2.0], [1.0, 14.0, 14.0, 2.0]),\n",
       " ([1.0, 5.0, 9.0, 6.0, 5.0, 2.0], [1.0, 16.0, 16.0, 16.0, 2.0]),\n",
       " ([1.0, 5.0, 6.0, 11.0, 2.0], [1.0, 17.0, 16.0, 2.0]),\n",
       " ([1.0, 5.0, 9.0, 2.0], [1.0, 16.0, 16.0, 2.0])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have uniform length\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one random example to treat as the query \n",
    "query_data = []\n",
    "\n",
    "for sample in tokenized_data:\n",
    "    random_sample = random.choice(sample)\n",
    "    # Add the selected tuple to query_data\n",
    "    query_data.append(random_sample)\n",
    "\n",
    "# query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 5.0, 4.0, 11.0, 3.0, 2.0], [1.0, 12.0, 16.0, 16.0, 17.0, 2.0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1.0, 4.0, 6.0, 2.0], [1.0, 17.0, 17.0, 2.0]),\n",
       " ([1.0, 3.0, 2.0], [1.0, 16.0, 2.0]),\n",
       " ([1.0, 8.0, 7.0, 5.0, 8.0, 2.0], [1.0, 12.0, 13.0, 13.0, 13.0, 2.0]),\n",
       " ([1.0, 8.0, 3.0, 2.0], [1.0, 13.0, 16.0, 2.0]),\n",
       " ([1.0, 5.0, 7.0, 5.0, 8.0, 2.0], [1.0, 12.0, 13.0, 12.0, 12.0, 2.0]),\n",
       " ([1.0, 8.0, 2.0], [1.0, 13.0, 2.0]),\n",
       " ([1.0, 5.0, 2.0], [1.0, 12.0, 2.0]),\n",
       " ([1.0, 3.0, 7.0, 3.0, 11.0, 8.0, 6.0, 2.0],\n",
       "  [1.0, 13.0, 13.0, 13.0, 13.0, 16.0, 16.0, 16.0, 2.0]),\n",
       " ([1.0, 4.0, 2.0], [1.0, 17.0, 2.0]),\n",
       " ([1.0, 5.0, 11.0, 3.0, 2.0], [1.0, 16.0, 16.0, 12.0, 2.0]),\n",
       " ([1.0, 5.0, 4.0, 11.0, 3.0, 2.0], [1.0, 12.0, 16.0, 16.0, 17.0, 2.0]),\n",
       " ([1.0, 5.0, 4.0, 11.0, 5.0, 6.0, 2.0],\n",
       "  [1.0, 12.0, 12.0, 12.0, 12.0, 12.0, 17.0, 2.0]),\n",
       " ([1.0, 3.0, 6.0, 2.0], [1.0, 16.0, 16.0, 2.0]),\n",
       " ([1.0, 5.0, 7.0, 4.0, 11.0, 5.0, 6.0, 2.0],\n",
       "  [1.0, 12.0, 12.0, 12.0, 12.0, 17.0, 12.0, 12.0, 2.0])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum input and output lengths across all sequences\n",
    "\n",
    "max_input_len = max(len(inp) for sublist in tokenized_data for inp, _ in sublist)\n",
    "max_output_len = max(len(out) for sublist in tokenized_data for _, out in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sublists\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Get the list of tuples from the corresponding sublist\n",
    "        index = self.data[idx]\n",
    "\n",
    "        if isinstance(index, list):\n",
    "            # tokenized_data is a list of a list of tuples\n",
    "            # Pad each input-output pair in the sublist\n",
    "            padded_data = [\n",
    "                (pad_sequence(inp, max_input_len), pad_sequence(out, max_output_len))\n",
    "                for inp, out in index\n",
    "            ]\n",
    "        else:\n",
    "            # query_data is a list of tuples\n",
    "            padded_data = [\n",
    "                (pad_sequence(index[0], max_input_len), pad_sequence(index[1], max_output_len))\n",
    "            ]\n",
    "\n",
    "        \n",
    "        # Convert each pair to a tensor\n",
    "        padded_tensors = [(torch.tensor(input_padded), torch.tensor(output_padded)) for input_padded, output_padded in padded_data]\n",
    "\n",
    "        return padded_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SequenceDataset(tokenized_data)\n",
    "query_dataset = SequenceDataset(query_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "query_dataloader = DataLoader(query_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 4., 7., 8., 2., 0., 0., 0., 0., 0.]),\n",
       " tensor([ 1., 13., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dataset[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([1., 3., 2., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 15.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 4., 7., 8., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 13., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([ 1.,  3.,  3.,  4., 11.,  2.,  0.,  0.,  0.,  0.]),\n",
       "  tensor([ 1., 15., 15., 13., 15., 15., 13.,  2.,  0.,  0.])),\n",
       " (tensor([1., 9., 7., 8., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 14., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 4., 8., 2., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 13., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 8., 2., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 8., 4., 3., 7., 9., 2., 0., 0., 0.]),\n",
       "  tensor([ 1., 16., 13., 15., 14.,  2.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 3., 7., 9., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 15., 14.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 8., 3., 7., 8., 2., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 16., 15., 16.,  2.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 4., 7., 3., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 13., 15.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 8., 7., 8., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 16., 16.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 8., 7., 9., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 16., 14.,  2.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 4., 2., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 13.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])),\n",
       " (tensor([1., 9., 7., 9., 2., 0., 0., 0., 0., 0.]),\n",
       "  tensor([ 1., 14., 14.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(tokens, token_to_symbol):\n",
    "    return [token_to_symbol[token] for token in tokens if token in token_to_symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1:\n",
      " Example 1:\n",
      "  Input tokens: [1.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'EOS']\n",
      " Example 2:\n",
      "  Input tokens: [1.0, 4.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'EOS']\n",
      " Example 3:\n",
      "  Input tokens: [1.0, 3.0, 3.0, 4.0, 11.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 15.0, 13.0, 15.0, 15.0, 13.0, 2.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'dax', 'lug', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'BLUE', 'YELLOW', 'BLUE', 'BLUE', 'YELLOW', 'EOS']\n",
      " Example 4:\n",
      "  Input tokens: [1.0, 9.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PURPLE', 'EOS']\n",
      " Example 5:\n",
      "  Input tokens: [1.0, 4.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'EOS']\n",
      " Example 6:\n",
      "  Input tokens: [1.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'EOS']\n",
      " Example 7:\n",
      "  Input tokens: [1.0, 8.0, 4.0, 3.0, 7.0, 9.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 13.0, 15.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'lug', 'dax', 'fep', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'YELLOW', 'BLUE', 'GREEN', 'EOS']\n",
      " Example 8:\n",
      "  Input tokens: [1.0, 3.0, 7.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'fep', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'GREEN', 'EOS']\n",
      " Example 9:\n",
      "  Input tokens: [1.0, 8.0, 3.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 15.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'dax', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'BLUE', 'PURPLE', 'EOS']\n",
      " Example 10:\n",
      "  Input tokens: [1.0, 4.0, 7.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'fep', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'BLUE', 'EOS']\n",
      " Example 11:\n",
      "  Input tokens: [1.0, 8.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 12:\n",
      "  Input tokens: [1.0, 8.0, 7.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'fep', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'GREEN', 'EOS']\n",
      " Example 13:\n",
      "  Input tokens: [1.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'EOS']\n",
      " Example 14:\n",
      "  Input tokens: [1.0, 9.0, 7.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'fep', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'GREEN', 'EOS']\n",
      "==================================================\n",
      "Group 2:\n",
      " Example 1:\n",
      "  Input tokens: [1.0, 4.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'PINK', 'EOS']\n",
      " Example 2:\n",
      "  Input tokens: [1.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'EOS']\n",
      " Example 3:\n",
      "  Input tokens: [1.0, 8.0, 7.0, 5.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 13.0, 13.0, 13.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'fep', 'wif', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'YELLOW', 'YELLOW', 'YELLOW', 'EOS']\n",
      " Example 4:\n",
      "  Input tokens: [1.0, 8.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'EOS']\n",
      " Example 5:\n",
      "  Input tokens: [1.0, 5.0, 7.0, 5.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 13.0, 12.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'fep', 'wif', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'YELLOW', 'RED', 'RED', 'EOS']\n",
      " Example 6:\n",
      "  Input tokens: [1.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'EOS']\n",
      " Example 7:\n",
      "  Input tokens: [1.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'EOS']\n",
      " Example 8:\n",
      "  Input tokens: [1.0, 3.0, 7.0, 3.0, 11.0, 8.0, 6.0, 2.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 13.0, 13.0, 13.0, 16.0, 16.0, 16.0, 2.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'fep', 'dax', 'gazzer', 'blicket', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'YELLOW', 'YELLOW', 'YELLOW', 'PURPLE', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 9:\n",
      "  Input tokens: [1.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'EOS']\n",
      " Example 10:\n",
      "  Input tokens: [1.0, 5.0, 11.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 16.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'gazzer', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'PURPLE', 'RED', 'EOS']\n",
      " Example 11:\n",
      "  Input tokens: [1.0, 5.0, 4.0, 11.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 16.0, 16.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'lug', 'gazzer', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'PURPLE', 'PURPLE', 'PINK', 'EOS']\n",
      " Example 12:\n",
      "  Input tokens: [1.0, 5.0, 4.0, 11.0, 5.0, 6.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 12.0, 12.0, 12.0, 12.0, 17.0, 2.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'lug', 'gazzer', 'wif', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'RED', 'RED', 'RED', 'RED', 'PINK', 'EOS']\n",
      " Example 13:\n",
      "  Input tokens: [1.0, 3.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 14:\n",
      "  Input tokens: [1.0, 5.0, 7.0, 4.0, 11.0, 5.0, 6.0, 2.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 12.0, 12.0, 12.0, 17.0, 12.0, 12.0, 2.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'fep', 'lug', 'gazzer', 'wif', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'RED', 'RED', 'RED', 'PINK', 'RED', 'RED', 'EOS']\n",
      "==================================================\n",
      "Group 3:\n",
      " Example 1:\n",
      "  Input tokens: [1.0, 10.0, 4.0, 11.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 14.0, 16.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'lug', 'gazzer', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'GREEN', 'PURPLE', 'GREEN', 'EOS']\n",
      " Example 2:\n",
      "  Input tokens: [1.0, 9.0, 4.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'lug', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'BLUE', 'EOS']\n",
      " Example 3:\n",
      "  Input tokens: [1.0, 9.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 15.0, 15.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'BLUE', 'BLUE', 'BLUE', 'EOS']\n",
      " Example 4:\n",
      "  Input tokens: [1.0, 11.0, 4.0, 10.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 16.0, 14.0, 16.0, 14.0, 16.0, 14.0, 16.0, 2.0]\n",
      "  Decoded Input: ['SOS', 'gazzer', 'lug', 'tufa', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PURPLE', 'GREEN', 'PURPLE', 'GREEN', 'PURPLE', 'GREEN', 'PURPLE', 'EOS']\n",
      " Example 5:\n",
      "  Input tokens: [1.0, 10.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 14.0, 14.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'GREEN', 'GREEN', 'GREEN', 'EOS']\n",
      " Example 6:\n",
      "  Input tokens: [1.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'EOS']\n",
      " Example 7:\n",
      "  Input tokens: [1.0, 11.0, 4.0, 10.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'gazzer', 'lug', 'tufa', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PURPLE', 'EOS']\n",
      " Example 8:\n",
      "  Input tokens: [1.0, 8.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'BLUE', 'EOS']\n",
      " Example 9:\n",
      "  Input tokens: [1.0, 10.0, 8.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 12.0, 14.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'blicket', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'RED', 'GREEN', 'RED', 'EOS']\n",
      " Example 10:\n",
      "  Input tokens: [1.0, 10.0, 4.0, 11.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'lug', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'GREEN', 'EOS']\n",
      " Example 11:\n",
      "  Input tokens: [1.0, 10.0, 4.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'lug', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'GREEN', 'EOS']\n",
      " Example 12:\n",
      "  Input tokens: [1.0, 10.0, 9.0, 4.0, 8.0, 6.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 12.0, 15.0, 14.0, 12.0, 15.0, 2.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'kiki', 'lug', 'blicket', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'RED', 'BLUE', 'GREEN', 'RED', 'BLUE', 'EOS']\n",
      " Example 13:\n",
      "  Input tokens: [1.0, 10.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'zup', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'GREEN', 'EOS']\n",
      " Example 14:\n",
      "  Input tokens: [1.0, 10.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'tufa', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'EOS']\n",
      "==================================================\n",
      "Group 4:\n",
      " Example 1:\n",
      "  Input tokens: [1.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'EOS']\n",
      " Example 2:\n",
      "  Input tokens: [1.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'EOS']\n",
      " Example 3:\n",
      "  Input tokens: [1.0, 5.0, 3.0, 7.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 13.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'dax', 'fep', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'YELLOW', 'PURPLE', 'EOS']\n",
      " Example 4:\n",
      "  Input tokens: [1.0, 3.0, 4.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 13.0, 17.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'lug', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'YELLOW', 'PINK', 'PINK', 'EOS']\n",
      " Example 5:\n",
      "  Input tokens: [1.0, 3.0, 8.0, 3.0, 11.0, 9.0, 5.0, 9.0, 7.0, 2.0]\n",
      "  Output tokens: [1.0, 16.0, 15.0, 13.0, 17.0, 13.0, 13.0, 13.0, 2.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'blicket', 'dax', 'gazzer', 'kiki', 'wif', 'kiki', 'fep', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'BLUE', 'YELLOW', 'PINK', 'YELLOW', 'YELLOW', 'YELLOW', 'EOS']\n",
      " Example 6:\n",
      "  Input tokens: [1.0, 5.0, 3.0, 11.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 13.0, 13.0, 13.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'dax', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'YELLOW', 'YELLOW', 'YELLOW', 'EOS']\n",
      " Example 7:\n",
      "  Input tokens: [1.0, 8.0, 9.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'kiki', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PINK', 'EOS']\n",
      " Example 8:\n",
      "  Input tokens: [1.0, 3.0, 7.0, 11.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'fep', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 9:\n",
      "  Input tokens: [1.0, 5.0, 4.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 15.0, 17.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'lug', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'BLUE', 'PINK', 'PINK', 'EOS']\n",
      " Example 10:\n",
      "  Input tokens: [1.0, 7.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'fep', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'EOS']\n",
      " Example 11:\n",
      "  Input tokens: [1.0, 7.0, 5.0, 4.0, 7.0, 9.0, 5.0, 2.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 16.0, 15.0, 15.0, 16.0, 16.0, 2.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'fep', 'wif', 'lug', 'fep', 'kiki', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'PURPLE', 'BLUE', 'BLUE', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 12:\n",
      "  Input tokens: [1.0, 3.0, 7.0, 4.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 16.0, 13.0, 13.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'fep', 'lug', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'PURPLE', 'YELLOW', 'YELLOW', 'EOS']\n",
      " Example 13:\n",
      "  Input tokens: [1.0, 3.0, 4.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 13.0, 15.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'lug', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'YELLOW', 'BLUE', 'BLUE', 'EOS']\n",
      " Example 14:\n",
      "  Input tokens: [1.0, 7.0, 4.0, 7.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 16.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'fep', 'lug', 'fep', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'PURPLE', 'PURPLE', 'PURPLE', 'EOS']\n",
      "==================================================\n",
      "Group 5:\n",
      " Example 1:\n",
      "  Input tokens: [1.0, 5.0, 7.0, 8.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 2.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'fep', 'blicket', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'PURPLE', 'PURPLE', 'PURPLE', 'PURPLE', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 2:\n",
      "  Input tokens: [1.0, 8.0, 7.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 14.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'fep', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'GREEN', 'GREEN', 'EOS']\n",
      " Example 3:\n",
      "  Input tokens: [1.0, 9.0, 7.0, 8.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 16.0, 14.0, 16.0, 14.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'fep', 'blicket', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PURPLE', 'GREEN', 'PURPLE', 'GREEN', 'EOS']\n",
      " Example 4:\n",
      "  Input tokens: [1.0, 9.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'EOS']\n",
      " Example 5:\n",
      "  Input tokens: [1.0, 8.0, 7.0, 8.0, 3.0, 5.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 12.0, 16.0, 12.0, 16.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'fep', 'blicket', 'dax', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'RED', 'PURPLE', 'RED', 'PURPLE', 'EOS']\n",
      " Example 6:\n",
      "  Input tokens: [1.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'EOS']\n",
      " Example 7:\n",
      "  Input tokens: [1.0, 11.0, 3.0, 9.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 17.0, 14.0, 17.0, 14.0, 17.0, 14.0, 17.0, 2.0]\n",
      "  Decoded Input: ['SOS', 'gazzer', 'dax', 'kiki', 'lug', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PINK', 'GREEN', 'PINK', 'GREEN', 'PINK', 'GREEN', 'PINK', 'EOS']\n",
      " Example 8:\n",
      "  Input tokens: [1.0, 5.0, 3.0, 9.0, 3.0, 11.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 14.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'dax', 'kiki', 'dax', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'GREEN', 'RED', 'EOS']\n",
      " Example 9:\n",
      "  Input tokens: [1.0, 5.0, 9.0, 7.0, 8.0, 3.0, 11.0, 2.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 12.0, 14.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'kiki', 'fep', 'blicket', 'dax', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'RED', 'GREEN', 'PURPLE', 'PURPLE', 'EOS']\n",
      " Example 10:\n",
      "  Input tokens: [1.0, 9.0, 11.0, 3.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 14.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'gazzer', 'dax', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'GREEN', 'PINK', 'EOS']\n",
      " Example 11:\n",
      "  Input tokens: [1.0, 11.0, 4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 17.0, 17.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'gazzer', 'lug', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'PINK', 'PINK', 'PINK', 'EOS']\n",
      " Example 12:\n",
      "  Input tokens: [1.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PURPLE', 'EOS']\n",
      " Example 13:\n",
      "  Input tokens: [1.0, 5.0, 3.0, 11.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 17.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'dax', 'gazzer', 'EOS']\n",
      "  Decoded Output: ['SOS', 'PINK', 'RED', 'EOS']\n",
      " Example 14:\n",
      "  Input tokens: [1.0, 9.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 14.0, 16.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'GREEN', 'PURPLE', 'PURPLE', 'EOS']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# View a few examples from the train_dataset\n",
    "for i in range(5):  # Print first 5 sublists/groups\n",
    "    print(f\"Group {i + 1}:\")\n",
    "    sublist = train_dataset[i]\n",
    "    for j, (input_tensor, output_tensor) in enumerate(sublist):\n",
    "        input_tokens = input_tensor.tolist()\n",
    "        output_tokens = output_tensor.tolist()\n",
    "        \n",
    "        print(f\" Example {j + 1}:\")\n",
    "        print(f\"  Input tokens: {input_tokens}\")\n",
    "        print(f\"  Output tokens: {output_tokens}\")\n",
    "\n",
    "        # Decode tokens to symbols for readability\n",
    "        input_symbols = decode_sequence(input_tokens, token_to_input_symbol)\n",
    "        output_symbols = decode_sequence(output_tokens, token_to_input_symbol)\n",
    "\n",
    "        print(f\"  Decoded Input: {input_symbols}\")\n",
    "        print(f\"  Decoded Output: {output_symbols}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1:\n",
      "  Input tokens: [1.0, 4.0, 7.0, 8.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 16.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'lug', 'fep', 'blicket', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'PURPLE', 'EOS']\n",
      "==================================================\n",
      "Group 2:\n",
      "  Input tokens: [1.0, 5.0, 4.0, 11.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 16.0, 16.0, 17.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'lug', 'gazzer', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'PURPLE', 'PURPLE', 'PINK', 'EOS']\n",
      "==================================================\n",
      "Group 3:\n",
      "  Input tokens: [1.0, 9.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 15.0, 15.0, 15.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'kiki', 'dax', 'EOS']\n",
      "  Decoded Output: ['SOS', 'BLUE', 'BLUE', 'BLUE', 'BLUE', 'EOS']\n",
      "==================================================\n",
      "Group 4:\n",
      "  Input tokens: [1.0, 3.0, 4.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 13.0, 13.0, 15.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'dax', 'lug', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'YELLOW', 'YELLOW', 'BLUE', 'BLUE', 'EOS']\n",
      "==================================================\n",
      "Group 5:\n",
      "  Input tokens: [1.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Output tokens: [1.0, 12.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Decoded Input: ['SOS', 'wif', 'EOS']\n",
      "  Decoded Output: ['SOS', 'RED', 'EOS']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# View a few examples from the query dataset\n",
    "for i in range(5):  # Print first 5 tuples\n",
    "    print(f\"Group {i + 1}:\")\n",
    "    input_tensor, output_tensor = query_dataset[i][0]\n",
    "    input_tokens = input_tensor.tolist()\n",
    "    output_tokens = output_tensor.tolist()\n",
    "        \n",
    "    print(f\"  Input tokens: {input_tokens}\")\n",
    "    print(f\"  Output tokens: {output_tokens}\")\n",
    "\n",
    "    # Decode tokens to symbols for readability\n",
    "    input_symbols = decode_sequence(input_tokens, token_to_input_symbol)\n",
    "    output_symbols = decode_sequence(output_tokens, token_to_input_symbol)\n",
    "\n",
    "    print(f\"  Decoded Input: {input_symbols}\")\n",
    "    print(f\"  Decoded Output: {output_symbols}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dimensions -> number of tokens \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "\n",
    "# what is the input size in a seq2seq?\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(input_list)\n",
    "hidden_size = 50\n",
    "output_size = len(output_list)\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n",
      "torch.Size([1, 32, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (num_epochs):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Each batch is a list of tuples, so you need to extract inputs and targets\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Track the loss for each batch\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Ensure seq and target are float tensors\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m     padded_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m         (pad_sequence(index[\u001b[38;5;241m0\u001b[39m], max_input_len), pad_sequence(index[\u001b[38;5;241m1\u001b[39m], max_output_len))\n\u001b[1;32m     25\u001b[0m     ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert each pair to a tensor\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m padded_tensors \u001b[38;5;241m=\u001b[39m [(torch\u001b[38;5;241m.\u001b[39mtensor(input_padded), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_padded\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m input_padded, output_padded \u001b[38;5;129;01min\u001b[39;00m padded_data]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padded_tensors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Each batch is a list of tuples, so you need to extract inputs and targets\n",
    "        batch_loss = 0.0  # Track the loss for each batch\n",
    "        print(batch.shape())\n",
    "        for seq, target in batch:\n",
    "            # Ensure seq and target are float tensors\n",
    "            seq = seq.float()\n",
    "            if seq.dim() == 2:\n",
    "                seq = seq.unsqueeze(0)\n",
    "                \n",
    "            target = target.float()\n",
    "            if (epoch == 0 and batch_idx == 0):\n",
    "                print(seq.shape)\n",
    "                # print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 11, got 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass through LSTM\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Take the output from the last time step\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:892\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    888\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m    889\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    890\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    891\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:821\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    817\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    818\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    819\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    820\u001b[0m                        ):\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    823\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    825\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:240\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 11, got 10"
     ]
    }
   ],
   "source": [
    "# Training loop using DataLoader\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        # Each batch is a list of tuples, so you need to extract inputs and targets\n",
    "        batch_loss = 0.0  # Track the loss for each batch\n",
    "        for seq, target in batch:\n",
    "            # Ensure seq and target are float tensors\n",
    "            seq = seq.float()\n",
    "            target = target.float()\n",
    "\n",
    "            # Add batch dimension if needed\n",
    "            if seq.dim() == 2:  # if not already batched\n",
    "                seq = seq.unsqueeze(0)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(seq)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            batch_loss += loss.item()  # Accumulate the loss for each sample\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {batch_loss:.4f}')\n",
    "\n",
    "# Test with a new sequence using the query_dataloader\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for test_batch in query_dataloader:\n",
    "        for test_seq, _ in test_batch:  # We are only interested in inputs here\n",
    "            # Ensure test_seq is a float tensor\n",
    "            test_seq = test_seq.float()\n",
    "\n",
    "            # Add batch dimension if needed\n",
    "            if test_seq.dim() == 2:  # if not already batched\n",
    "                test_seq = test_seq.unsqueeze(0)\n",
    "\n",
    "            predicted_output = model(test_seq)\n",
    "            print(f'Test sequence: {test_seq.squeeze().tolist()}')\n",
    "            print(f'Predicted next value: {predicted_output.squeeze().item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
